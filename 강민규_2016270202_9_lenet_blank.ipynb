{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "9_lenet_blank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CyOHNw1elc9"
      },
      "source": [
        "#**Mounting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6CRox2VekDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0af6de2-e96c-4e54-eab5-c2bc0e00f9cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UZrMeB0erXZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4a013e6-3f49-46b7-a46a-63e7f6e9c0ae"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/ML_class/CNN')\n",
        "%pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/ML_class/CNN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tCipQ-4nGpr"
      },
      "source": [
        "# Convolutional Neural Networks: MNIST\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "- Implement helper functions that you will use when implementing a PyTorch model\n",
        "- Implement a fully functioning ConvNet using PyTorch \n",
        "\n",
        "**After this assignment you will be able to:**\n",
        "\n",
        "- Build and train a ConvNet in PyTorch for a classification problem \n",
        "\n",
        "We assume here that you are already familiar with PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aatpvdbgms50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6705d30e-43f7-44f1-9145-818622a9a484"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('using device:', device)\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2,1), 'MB')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda\n",
            "Tesla T4\n",
            "Memory usage:\n",
            "Allocated: 42.4 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKpoUNNnQxA"
      },
      "source": [
        "## 1. PyTorch model\n",
        "\n",
        "### 1.1. Load dataset (MNIST) \n",
        "\n",
        "Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. \n",
        "\n",
        "We will start by loading in the packages. \n",
        "\n",
        "The MNIST Dataset is located at http://yann.lecun.com/exdb/mnist/. Each image has $28 \\times 28$ dimension.\n",
        "\n",
        "-train-images-idx3-ubyte.gz:  training set images (9912422 bytes) including 55000 examples <br>\n",
        "-train-labels-idx1-ubyte.gz:  training set labels (28881 bytes) <br>\n",
        "-t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes) including 10000 examples <br>\n",
        "-t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFF0e5udms54"
      },
      "source": [
        "train_dataset = datasets.MNIST( root='./mnist_data/',\n",
        "                              train=True,\n",
        "                              transform=transforms.ToTensor(),\n",
        "                              download=True)\n",
        "test_dataset = datasets.MNIST( root='./mnist_data/',\n",
        "                             train = False,\n",
        "                             transform=transforms.ToTensor())"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwkQNi5foOTO"
      },
      "source": [
        "### 1.1. Prepare dataset\n",
        "\n",
        "Let's prepare the dataset using **torch.tuils.data.Dataloader**\n",
        "\n",
        "PyTorch provides DataLoader for the input data (training/testing) that will be fed into the model when training/testing the model.\n",
        "\n",
        "**Exercise**: Implement the function below to prepare the training/testing data. You can define the number of examples for the moment and shuffle the order of examples by setting batch_size and shuffle.\n",
        "\n",
        "**The constructor arguments of a DataLoader :**\n",
        "```python\n",
        "torch.utils.data.DataLoader (dataset, batch_size=1, shuffle=False, sampler=None,\n",
        "                            batch_sampler=None, num_workers=0, collate_fn=None,\n",
        "                            pin_memory=False, drop_last=False, timeout=0,\n",
        "                            worker_init_fn=None)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNXScGRoIEb"
      },
      "source": [
        "### Set the batch_size \n",
        "batch_size = 64\n",
        "\n",
        "### START CODE HERE ### (≈1 lines)\n",
        "#Use torch.utils.data.DataLoader() with bath_size 64, shuffle = True \n",
        "train_loader = torch.utils.data.DataLoader( dataset=train_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle=True)\n",
        "### END CODE HERE ###  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader( dataset=test_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "# train_loader has (x, y) where x.size()=[64, 1, 28, 28] and y.size()=[64]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpdTftvIpRFs"
      },
      "source": [
        "### 1.2. Forward propagation\n",
        "\n",
        "In PyTorch, there are built-in functions that carry out the convolution steps for you.\n",
        "\n",
        "- **torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):** Applies a 2D convolution over an input signal composed of several input planes.  \n",
        "\n",
        "- **torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):** applies a 2D max pooling over an input signal composed of several input planes.\n",
        "\n",
        "- **torch.nn.Linear(in_features, out_features, bias=True):** Applies a linear transformation to the incoming data\n",
        "\n",
        "- **torch.nn.functional.relu(input, inplace=False):** Applies the rectified linear unit function element-wise \n",
        "\n",
        "- **torch.nn.functional.softmax(dim=None):** torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "\n",
        "- **torch.flatten(input, start_dim=0, end_dim=-1) :** Flattens a contiguous range of dims in a tensor.\n",
        "\n",
        "**For more details:**\n",
        "\n",
        "torch.nn : https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "torch.functional : https://pytorch.org/docs/stable/nn.functional.html\n",
        "\n",
        "torch.flatten : https://pytorch.org/docs/master/generated/torch.flatten.html\n",
        "\n",
        "\n",
        "**Exercise**: \n",
        "\n",
        "Implement the `NNModel` class below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bDY8CEgms6f"
      },
      "source": [
        "\"\"\"\n",
        "Implements forward propabation for the CNN model:\n",
        "CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
        "\n",
        "\"\"\"\n",
        "class NNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, padding=(2,2)) # input channel, output channels, and filter size\n",
        "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Define the Convolution layer(Condv2d) with (32:input channel, 64:output channels, 5: filter size) and the 2 padding. \n",
        "        self.conv2 = nn.Conv2d(32,64,5,padding=2)  \n",
        "        # Define the Maxpooling layer(MaxPool2d) with (2:filter size, 2:stride)\n",
        "        self.pool2 = nn.MaxPool2d(2,stride=2)\n",
        "        # Define the fully-conneted layer(Linear) with (3136:input features, 1024:output features)\n",
        "        self.fc1 = nn.Linear(3136,1024)\n",
        "        ### END CODE HERE ###\n",
        "        self.fc2 = nn.Linear(1024,10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Arguments:\n",
        "            X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "            parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
        "                          the shapes are given in initialize_parameters\n",
        "\n",
        "            Returns:\n",
        "            F.softmax(x) -- the softmax output of the last LINEAR unit\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        ### START CODE HERE ###\n",
        "        # Load sceond Convoultion layer(conv2d) \n",
        "        x = self.conv2(x)\n",
        "        # Load Relu function  \n",
        "        x = F.relu(x)\n",
        "        # Load sceond Maxpooling layer(MaxPool2d) \n",
        "        x = self.pool2(x)\n",
        "        # Flatten each example into a 1D vector: (?,7,7,64)->(?, 3136)\n",
        "        x = x.view(-1,3136)\n",
        "        # Load first fully-connted layer \n",
        "        x = self.fc1(x) \n",
        "        ### END CODE HERE ### \n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x)    \n",
        "    "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvmWxnal0hWA"
      },
      "source": [
        "### 1.3. Build model\n",
        "\n",
        "Finally you will merge the helper functions you implemented above to build and train a model.  \n",
        "\n",
        "**Exercise**: Complete the function below. \n",
        "\n",
        "The model below should:\n",
        "\n",
        "- Define the cost function with **torch.nn.CrossEntropyLoss()**    \n",
        "> torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') \n",
        "> **For more details:** https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "- Create optmizization function with **torch.optim.SGD()** \n",
        "> torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)     \n",
        "> **For more details:** https://pytorch.org/docs/stable/optim.html\n",
        "- hint : For getting params for optmizer, use the following code\n",
        "```python\n",
        "params = model.parameters()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "akEyMJMxms6i"
      },
      "source": [
        "model = NNModel()\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "model.apply(weights_init)\n",
        "\n",
        "### START CODE HERE ###  \n",
        "# Define the cost function with cross entropy      \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Create optimizer \n",
        "# Get model paramters \n",
        "params = model.parameters()\n",
        "# Define the optimizer (using SGD) with learing rate=0.01 and momentum=0.5\n",
        "optimizer = torch.optim.SGD(params,lr=0.01 ,momentum=0.5)\n",
        "### END CODE HERE ###\n",
        "\n",
        "model = model.to(device)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701_v22n3KxE"
      },
      "source": [
        "## 2. Train and test model\n",
        "\n",
        "Finally you will create training and testing function with a loop for each mini-batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-cfYlJx4ig1"
      },
      "source": [
        "### 2.1 Train function\n",
        "\n",
        "Implement the function to train the CNN model.\n",
        "\n",
        "**Exercise**: Complete the function below. \n",
        "\n",
        "- copy a tensor on the CPU to the GPU \n",
        "- call the CNN model \n",
        "- initialize the optimizer  \n",
        "- compute the cost  \n",
        "- compute the backward propagation\n",
        "- update parameters with the optimizer \n",
        "\n",
        "**Hint**\n",
        "- copy a tensor on the CPU to the GPU\n",
        "         # copy a tensor to the CPU \n",
        "         data = data.to(\"cpu\")\n",
        "         # copy a tensor to the GPU \n",
        "         data = data.to(\"cuda\") \n",
        "- call the CNN model : call the predefined CNN model funciton    \n",
        "- initialize optimizer \n",
        "        optimizer.zero_grad() \n",
        "- compute the cost : call the predifined criterion function  \n",
        "- compute the backward propagation   \n",
        "        loss.backward()\n",
        "- update parameters with the optimizer \n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40y8dtLi24_0"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        ### START CODE HERE ###  \n",
        "        # Copy \"data\" tensor to the GPU\n",
        "        data = data.to(\"cuda\") \n",
        "        # Copy \"target\" tensor to the GPU\n",
        "        target = target.to(\"cuda\") \n",
        "        # Call model function and input the data tensor \n",
        "        output = model(data)                                ##과연\n",
        "        # Initialize the optimizer  \n",
        "        optimizer.zero_grad()  \n",
        "        # Compute the cost \n",
        "        loss = criterion(output,target)\n",
        "        # Compute the back-prop\n",
        "        loss.backward()\n",
        "        # Update prameters with the optmizer \n",
        "        optimizer.step()\n",
        "        ### END CODE HERE ###\n",
        "        if batch_idx%100==0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, \n",
        "                batch_idx*len(data), \n",
        "                len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), \n",
        "                loss))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqMzKBR_4xCq"
      },
      "source": [
        "### 2.2 Test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXIwfghqms6n"
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss=0\n",
        "        correct = 0\n",
        "        for data, target in train_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output,target)\n",
        "            pred = output.data.max(1,keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    \n",
        "        test_loss /=len(train_loader.dataset)\n",
        "        print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "            test_loss, correct, \n",
        "            len(train_loader.dataset),\n",
        "            100. * correct / len(train_loader.dataset)))\n",
        "\n",
        "        test_loss=0\n",
        "        correct = 0\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output,target)\n",
        "            pred = output.data.max(1,keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    \n",
        "        test_loss /=len(test_loader.dataset)\n",
        "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, \n",
        "            len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xleeHtag4DKl"
      },
      "source": [
        "### 2.3. Run the train/test function\n",
        "\n",
        "Run the following cell to train and test your model for 100 epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5McB7DF1D5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232d613b-2bc8-4ef2-e659-34f8b2831739"
      },
      "source": [
        "for epoch in range(0,100):\n",
        "    train(epoch)\n",
        "    if epoch%100==0:\n",
        "        test()\n",
        "test()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302899\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.302078\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.301781\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.301883\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.301391\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.300914\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.300801\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 2.298971\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.299407\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 2.296929\n",
            "Train set: Average loss: 0.0359, Accuracy: 21400/60000 (36%)\n",
            "Test set: Average loss: 0.0361, Accuracy: 3666/10000 (37%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297214\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.295364\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.294090\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.296659\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.288916\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.269255\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.186117\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.078169\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.890103\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.727811\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.782761\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.689210\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.669556\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.682265\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.652083\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.655463\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.595260\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.682868\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.586098\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.671798\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.712602\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.609596\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.680604\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.669340\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.613193\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.626572\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.639643\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.697101\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.661967\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.588480\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.625679\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.566848\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.624744\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.590916\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.644064\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.584616\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.624345\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.643051\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.618931\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.547405\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.610112\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.578445\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.570111\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.633184\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.642468\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.635724\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.635878\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.606721\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.571852\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.573969\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.551300\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.585089\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.512018\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.555007\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.486912\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.525818\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.555696\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.537508\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.506516\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.530126\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.502341\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.515867\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.560615\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.502680\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.504442\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.490858\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.489873\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.505149\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.509487\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.497483\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.494949\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.558298\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.527155\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.503489\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.481308\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.556304\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.517700\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.527583\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.486936\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.507018\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.516666\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.528590\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.537970\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.497994\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.473320\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.506528\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.480592\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.530948\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.487600\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.479877\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.477376\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.503995\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.494946\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.514120\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.497464\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.490840\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.512461\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.464517\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.475641\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.530957\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.504095\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 1.490015\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 1.499669\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 1.496035\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.489831\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 1.479740\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 1.479833\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 1.518874\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.498488\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 1.494574\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.480153\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 1.487756\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 1.473531\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 1.489182\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.482221\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 1.501007\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 1.494295\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 1.471260\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.479203\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 1.489717\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.469458\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 1.516965\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 1.528736\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 1.462922\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.497728\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 1.480013\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 1.477277\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 1.464886\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 1.478791\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 1.484806\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.490687\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 1.479370\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 1.492691\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 1.462988\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.479921\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 1.490128\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 1.478544\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 1.482498\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.489386\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 1.514877\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.475702\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 1.491481\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 1.477835\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 1.498883\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.472896\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 1.461922\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 1.468396\n",
            "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 1.470101\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.498352\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 1.479239\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.479102\n",
            "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 1.474021\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 1.500715\n",
            "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 1.495631\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.461504\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 1.479961\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 1.505009\n",
            "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 1.470973\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.482357\n",
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 1.469115\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.495558\n",
            "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 1.527256\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 1.504777\n",
            "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 1.497165\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 1.478324\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 1.483170\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 1.468857\n",
            "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 1.472615\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 1.484630\n",
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 1.484296\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.470104\n",
            "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 1.477389\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 1.495357\n",
            "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 1.510313\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.478618\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 1.465556\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 1.480495\n",
            "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 1.513654\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.491983\n",
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 1.481562\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.463413\n",
            "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 1.478612\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 1.465109\n",
            "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 1.476681\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.485500\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 1.482835\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 1.488258\n",
            "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 1.491499\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 1.464149\n",
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 1.470663\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.475697\n",
            "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 1.479773\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 1.478011\n",
            "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 1.482462\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 1.483041\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 1.475488\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 1.461448\n",
            "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 1.480675\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 1.477587\n",
            "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 1.472598\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 1.546127\n",
            "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 1.463582\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 1.473621\n",
            "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 1.493374\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 1.469294\n",
            "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 1.492192\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 1.494031\n",
            "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 1.508008\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 1.500165\n",
            "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 1.515237\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 1.493679\n",
            "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 1.462880\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 1.475876\n",
            "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 1.478097\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 1.496889\n",
            "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 1.483916\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 1.461479\n",
            "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 1.483361\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 1.472279\n",
            "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 1.463191\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 1.469704\n",
            "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 1.477027\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 1.474678\n",
            "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 1.479825\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 1.464181\n",
            "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 1.475549\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 1.477593\n",
            "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 1.489298\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 1.465728\n",
            "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 1.492908\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 1.464343\n",
            "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 1.484733\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 1.487534\n",
            "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 1.462424\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 1.476408\n",
            "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 1.490286\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 1.469856\n",
            "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 1.474407\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 1.478419\n",
            "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 1.507831\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 1.480782\n",
            "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 1.475886\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 1.461380\n",
            "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 1.491684\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 1.468460\n",
            "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 1.469232\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 1.513381\n",
            "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 1.461406\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 1.475098\n",
            "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 1.477543\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 1.480102\n",
            "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 1.462974\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 1.489059\n",
            "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 1.470152\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 1.503759\n",
            "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 1.481660\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 1.499404\n",
            "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 1.464436\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 1.503758\n",
            "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 1.463259\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 1.462374\n",
            "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 1.464621\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 1.461612\n",
            "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 1.463507\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 1.503329\n",
            "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 1.479349\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 1.462238\n",
            "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 1.492805\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 1.496462\n",
            "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 1.491529\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 1.476943\n",
            "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 1.479957\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 1.462756\n",
            "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 1.462460\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 1.462088\n",
            "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 1.462973\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 1.479094\n",
            "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 1.469197\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 1.462044\n",
            "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 1.470095\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 1.470957\n",
            "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 1.461688\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 1.462199\n",
            "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 1.463731\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 1.483529\n",
            "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 1.477513\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 1.483665\n",
            "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 1.492553\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 1.481167\n",
            "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 1.477994\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 1.470414\n",
            "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 1.477149\n",
            "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 1.476820\n",
            "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 1.477184\n",
            "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 1.473880\n",
            "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 1.500768\n",
            "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 1.461513\n",
            "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 1.464430\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 1.469101\n",
            "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 1.461460\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 1.475546\n",
            "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 1.477958\n",
            "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 1.462019\n",
            "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 1.492903\n",
            "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 1.481946\n",
            "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 1.461479\n",
            "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 1.465674\n",
            "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 1.493683\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 1.467193\n",
            "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 1.505353\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 1.484867\n",
            "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 1.461611\n",
            "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 1.464130\n",
            "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 1.474897\n",
            "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 1.481019\n",
            "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 1.463126\n",
            "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 1.468289\n",
            "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 1.464965\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 1.481439\n",
            "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 1.473537\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 1.461399\n",
            "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 1.462398\n",
            "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 1.461754\n",
            "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 1.464806\n",
            "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 1.496905\n",
            "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 1.475469\n",
            "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 1.461860\n",
            "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 1.480660\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 1.476953\n",
            "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 1.470062\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 1.462210\n",
            "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 1.477228\n",
            "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 1.473581\n",
            "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 1.465085\n",
            "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 1.461510\n",
            "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 1.521326\n",
            "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 1.464733\n",
            "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 1.474447\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 1.462102\n",
            "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 1.478465\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 1.476585\n",
            "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 1.483251\n",
            "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 1.476976\n",
            "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 1.471059\n",
            "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 1.491486\n",
            "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 1.465977\n",
            "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 1.463662\n",
            "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 1.479054\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 1.462900\n",
            "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 1.462436\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 1.478944\n",
            "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 1.476916\n",
            "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 1.461879\n",
            "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 1.462505\n",
            "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 1.462939\n",
            "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 1.462687\n",
            "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 1.463138\n",
            "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 1.475963\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 1.464330\n",
            "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 1.465602\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 1.469827\n",
            "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 1.461249\n",
            "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 1.463656\n",
            "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 1.473012\n",
            "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 1.480318\n",
            "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 1.464639\n",
            "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 1.475259\n",
            "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 1.479661\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 1.501392\n",
            "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 1.463476\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 1.477092\n",
            "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 1.483113\n",
            "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 1.482595\n",
            "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 1.508220\n",
            "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 1.478657\n",
            "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 1.465110\n",
            "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 1.467168\n",
            "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 1.461579\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 1.464872\n",
            "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 1.463699\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 1.471778\n",
            "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 1.468378\n",
            "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 1.461392\n",
            "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 1.477491\n",
            "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 1.480908\n",
            "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 1.463383\n",
            "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 1.462181\n",
            "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 1.462045\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 1.463283\n",
            "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 1.466088\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 1.470837\n",
            "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 1.476794\n",
            "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 1.465479\n",
            "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 1.462488\n",
            "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 1.478379\n",
            "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 1.461468\n",
            "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 1.491406\n",
            "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 1.461906\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 1.491908\n",
            "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 1.474761\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 1.472997\n",
            "Train Epoch: 41 [6400/60000 (11%)]\tLoss: 1.466284\n",
            "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 1.492189\n",
            "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 1.475388\n",
            "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 1.477431\n",
            "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 1.473234\n",
            "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 1.469598\n",
            "Train Epoch: 41 [44800/60000 (75%)]\tLoss: 1.481262\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 1.461303\n",
            "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 1.489945\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 1.466852\n",
            "Train Epoch: 42 [6400/60000 (11%)]\tLoss: 1.477463\n",
            "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 1.461246\n",
            "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 1.476431\n",
            "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 1.464887\n",
            "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 1.464399\n",
            "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 1.476259\n",
            "Train Epoch: 42 [44800/60000 (75%)]\tLoss: 1.461555\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 1.464633\n",
            "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 1.463597\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 1.463571\n",
            "Train Epoch: 43 [6400/60000 (11%)]\tLoss: 1.484430\n",
            "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 1.474451\n",
            "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 1.467725\n",
            "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 1.461905\n",
            "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 1.479908\n",
            "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 1.476833\n",
            "Train Epoch: 43 [44800/60000 (75%)]\tLoss: 1.468866\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 1.462674\n",
            "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 1.476043\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 1.478062\n",
            "Train Epoch: 44 [6400/60000 (11%)]\tLoss: 1.461452\n",
            "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 1.461907\n",
            "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 1.494241\n",
            "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 1.461413\n",
            "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 1.461550\n",
            "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 1.467389\n",
            "Train Epoch: 44 [44800/60000 (75%)]\tLoss: 1.478954\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 1.482472\n",
            "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 1.482979\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 1.461280\n",
            "Train Epoch: 45 [6400/60000 (11%)]\tLoss: 1.475474\n",
            "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 1.492500\n",
            "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 1.462535\n",
            "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 1.461456\n",
            "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 1.466928\n",
            "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 1.464491\n",
            "Train Epoch: 45 [44800/60000 (75%)]\tLoss: 1.470803\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 1.486980\n",
            "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 1.461380\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 1.461313\n",
            "Train Epoch: 46 [6400/60000 (11%)]\tLoss: 1.478850\n",
            "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 1.470521\n",
            "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 1.462519\n",
            "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 1.465349\n",
            "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 1.490990\n",
            "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 1.462343\n",
            "Train Epoch: 46 [44800/60000 (75%)]\tLoss: 1.462483\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 1.462075\n",
            "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 1.481199\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 1.462783\n",
            "Train Epoch: 47 [6400/60000 (11%)]\tLoss: 1.476539\n",
            "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 1.472416\n",
            "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 1.467585\n",
            "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 1.462626\n",
            "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 1.475725\n",
            "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 1.465071\n",
            "Train Epoch: 47 [44800/60000 (75%)]\tLoss: 1.461998\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 1.461155\n",
            "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 1.465343\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 1.498596\n",
            "Train Epoch: 48 [6400/60000 (11%)]\tLoss: 1.461154\n",
            "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 1.492719\n",
            "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 1.461498\n",
            "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 1.461922\n",
            "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 1.462407\n",
            "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 1.482350\n",
            "Train Epoch: 48 [44800/60000 (75%)]\tLoss: 1.461270\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 1.477537\n",
            "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 1.472008\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 1.477019\n",
            "Train Epoch: 49 [6400/60000 (11%)]\tLoss: 1.483487\n",
            "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 1.473134\n",
            "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 1.461601\n",
            "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 1.477391\n",
            "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 1.461757\n",
            "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 1.467757\n",
            "Train Epoch: 49 [44800/60000 (75%)]\tLoss: 1.462420\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 1.461217\n",
            "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 1.461157\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 1.461182\n",
            "Train Epoch: 50 [6400/60000 (11%)]\tLoss: 1.462773\n",
            "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 1.461425\n",
            "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 1.479420\n",
            "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 1.474543\n",
            "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 1.461258\n",
            "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 1.492762\n",
            "Train Epoch: 50 [44800/60000 (75%)]\tLoss: 1.464243\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 1.461347\n",
            "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 1.469155\n",
            "Train Epoch: 51 [0/60000 (0%)]\tLoss: 1.461255\n",
            "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 1.462267\n",
            "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 1.471818\n",
            "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 1.461303\n",
            "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 1.461239\n",
            "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 1.461182\n",
            "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 1.462600\n",
            "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 1.477651\n",
            "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 1.466810\n",
            "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 1.463453\n",
            "Train Epoch: 52 [0/60000 (0%)]\tLoss: 1.479035\n",
            "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 1.465584\n",
            "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 1.461565\n",
            "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 1.461378\n",
            "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 1.461516\n",
            "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 1.472629\n",
            "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 1.483662\n",
            "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 1.462203\n",
            "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 1.461665\n",
            "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 1.461378\n",
            "Train Epoch: 53 [0/60000 (0%)]\tLoss: 1.461365\n",
            "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 1.473942\n",
            "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 1.467154\n",
            "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 1.476289\n",
            "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 1.464224\n",
            "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 1.461160\n",
            "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 1.461815\n",
            "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 1.461604\n",
            "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 1.461179\n",
            "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 1.462293\n",
            "Train Epoch: 54 [0/60000 (0%)]\tLoss: 1.461374\n",
            "Train Epoch: 54 [6400/60000 (11%)]\tLoss: 1.469544\n",
            "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 1.500106\n",
            "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 1.465808\n",
            "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 1.461204\n",
            "Train Epoch: 54 [32000/60000 (53%)]\tLoss: 1.487870\n",
            "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 1.466526\n",
            "Train Epoch: 54 [44800/60000 (75%)]\tLoss: 1.500992\n",
            "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 1.461669\n",
            "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 1.462746\n",
            "Train Epoch: 55 [0/60000 (0%)]\tLoss: 1.461684\n",
            "Train Epoch: 55 [6400/60000 (11%)]\tLoss: 1.476341\n",
            "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 1.462190\n",
            "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 1.463157\n",
            "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 1.470266\n",
            "Train Epoch: 55 [32000/60000 (53%)]\tLoss: 1.465346\n",
            "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 1.461926\n",
            "Train Epoch: 55 [44800/60000 (75%)]\tLoss: 1.476843\n",
            "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 1.466949\n",
            "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 1.483537\n",
            "Train Epoch: 56 [0/60000 (0%)]\tLoss: 1.472168\n",
            "Train Epoch: 56 [6400/60000 (11%)]\tLoss: 1.473859\n",
            "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 1.462428\n",
            "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 1.461557\n",
            "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 1.461915\n",
            "Train Epoch: 56 [32000/60000 (53%)]\tLoss: 1.461241\n",
            "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 1.462422\n",
            "Train Epoch: 56 [44800/60000 (75%)]\tLoss: 1.471853\n",
            "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 1.479183\n",
            "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 1.461617\n",
            "Train Epoch: 57 [0/60000 (0%)]\tLoss: 1.461359\n",
            "Train Epoch: 57 [6400/60000 (11%)]\tLoss: 1.483009\n",
            "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 1.461686\n",
            "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 1.466956\n",
            "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 1.461153\n",
            "Train Epoch: 57 [32000/60000 (53%)]\tLoss: 1.473486\n",
            "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 1.477866\n",
            "Train Epoch: 57 [44800/60000 (75%)]\tLoss: 1.461437\n",
            "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 1.461303\n",
            "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 1.462482\n",
            "Train Epoch: 58 [0/60000 (0%)]\tLoss: 1.461166\n",
            "Train Epoch: 58 [6400/60000 (11%)]\tLoss: 1.461326\n",
            "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 1.461500\n",
            "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 1.466318\n",
            "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 1.461953\n",
            "Train Epoch: 58 [32000/60000 (53%)]\tLoss: 1.477223\n",
            "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 1.462714\n",
            "Train Epoch: 58 [44800/60000 (75%)]\tLoss: 1.492689\n",
            "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 1.484146\n",
            "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 1.463449\n",
            "Train Epoch: 59 [0/60000 (0%)]\tLoss: 1.462134\n",
            "Train Epoch: 59 [6400/60000 (11%)]\tLoss: 1.464064\n",
            "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 1.461239\n",
            "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 1.477151\n",
            "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 1.461604\n",
            "Train Epoch: 59 [32000/60000 (53%)]\tLoss: 1.461507\n",
            "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 1.463106\n",
            "Train Epoch: 59 [44800/60000 (75%)]\tLoss: 1.461291\n",
            "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 1.476699\n",
            "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 1.462248\n",
            "Train Epoch: 60 [0/60000 (0%)]\tLoss: 1.480154\n",
            "Train Epoch: 60 [6400/60000 (11%)]\tLoss: 1.462050\n",
            "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 1.463719\n",
            "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 1.462539\n",
            "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 1.461494\n",
            "Train Epoch: 60 [32000/60000 (53%)]\tLoss: 1.461404\n",
            "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 1.461897\n",
            "Train Epoch: 60 [44800/60000 (75%)]\tLoss: 1.462590\n",
            "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 1.462344\n",
            "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 1.474289\n",
            "Train Epoch: 61 [0/60000 (0%)]\tLoss: 1.487455\n",
            "Train Epoch: 61 [6400/60000 (11%)]\tLoss: 1.461990\n",
            "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 1.476787\n",
            "Train Epoch: 61 [19200/60000 (32%)]\tLoss: 1.461806\n",
            "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 1.462805\n",
            "Train Epoch: 61 [32000/60000 (53%)]\tLoss: 1.476853\n",
            "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 1.477211\n",
            "Train Epoch: 61 [44800/60000 (75%)]\tLoss: 1.477516\n",
            "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 1.461769\n",
            "Train Epoch: 61 [57600/60000 (96%)]\tLoss: 1.478832\n",
            "Train Epoch: 62 [0/60000 (0%)]\tLoss: 1.461181\n",
            "Train Epoch: 62 [6400/60000 (11%)]\tLoss: 1.463585\n",
            "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 1.461550\n",
            "Train Epoch: 62 [19200/60000 (32%)]\tLoss: 1.463751\n",
            "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 1.462451\n",
            "Train Epoch: 62 [32000/60000 (53%)]\tLoss: 1.461430\n",
            "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 1.461291\n",
            "Train Epoch: 62 [44800/60000 (75%)]\tLoss: 1.478480\n",
            "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 1.467869\n",
            "Train Epoch: 62 [57600/60000 (96%)]\tLoss: 1.461328\n",
            "Train Epoch: 63 [0/60000 (0%)]\tLoss: 1.463358\n",
            "Train Epoch: 63 [6400/60000 (11%)]\tLoss: 1.477130\n",
            "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 1.461185\n",
            "Train Epoch: 63 [19200/60000 (32%)]\tLoss: 1.461183\n",
            "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 1.463670\n",
            "Train Epoch: 63 [32000/60000 (53%)]\tLoss: 1.477776\n",
            "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 1.461165\n",
            "Train Epoch: 63 [44800/60000 (75%)]\tLoss: 1.461673\n",
            "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 1.476774\n",
            "Train Epoch: 63 [57600/60000 (96%)]\tLoss: 1.462591\n",
            "Train Epoch: 64 [0/60000 (0%)]\tLoss: 1.461235\n",
            "Train Epoch: 64 [6400/60000 (11%)]\tLoss: 1.461586\n",
            "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 1.476728\n",
            "Train Epoch: 64 [19200/60000 (32%)]\tLoss: 1.462572\n",
            "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 1.466357\n",
            "Train Epoch: 64 [32000/60000 (53%)]\tLoss: 1.470016\n",
            "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 1.461902\n",
            "Train Epoch: 64 [44800/60000 (75%)]\tLoss: 1.466121\n",
            "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 1.470319\n",
            "Train Epoch: 64 [57600/60000 (96%)]\tLoss: 1.475446\n",
            "Train Epoch: 65 [0/60000 (0%)]\tLoss: 1.464024\n",
            "Train Epoch: 65 [6400/60000 (11%)]\tLoss: 1.476920\n",
            "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 1.462430\n",
            "Train Epoch: 65 [19200/60000 (32%)]\tLoss: 1.461817\n",
            "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 1.461225\n",
            "Train Epoch: 65 [32000/60000 (53%)]\tLoss: 1.461416\n",
            "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 1.491851\n",
            "Train Epoch: 65 [44800/60000 (75%)]\tLoss: 1.463042\n",
            "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 1.467183\n",
            "Train Epoch: 65 [57600/60000 (96%)]\tLoss: 1.461386\n",
            "Train Epoch: 66 [0/60000 (0%)]\tLoss: 1.473477\n",
            "Train Epoch: 66 [6400/60000 (11%)]\tLoss: 1.461280\n",
            "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 1.461214\n",
            "Train Epoch: 66 [19200/60000 (32%)]\tLoss: 1.477803\n",
            "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 1.477425\n",
            "Train Epoch: 66 [32000/60000 (53%)]\tLoss: 1.477729\n",
            "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 1.481058\n",
            "Train Epoch: 66 [44800/60000 (75%)]\tLoss: 1.462245\n",
            "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 1.462237\n",
            "Train Epoch: 66 [57600/60000 (96%)]\tLoss: 1.462273\n",
            "Train Epoch: 67 [0/60000 (0%)]\tLoss: 1.461184\n",
            "Train Epoch: 67 [6400/60000 (11%)]\tLoss: 1.463884\n",
            "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 1.464628\n",
            "Train Epoch: 67 [19200/60000 (32%)]\tLoss: 1.461772\n",
            "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 1.478997\n",
            "Train Epoch: 67 [32000/60000 (53%)]\tLoss: 1.461369\n",
            "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 1.461343\n",
            "Train Epoch: 67 [44800/60000 (75%)]\tLoss: 1.477695\n",
            "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 1.477183\n",
            "Train Epoch: 67 [57600/60000 (96%)]\tLoss: 1.462132\n",
            "Train Epoch: 68 [0/60000 (0%)]\tLoss: 1.461772\n",
            "Train Epoch: 68 [6400/60000 (11%)]\tLoss: 1.478380\n",
            "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 1.461388\n",
            "Train Epoch: 68 [19200/60000 (32%)]\tLoss: 1.461175\n",
            "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 1.461408\n",
            "Train Epoch: 68 [32000/60000 (53%)]\tLoss: 1.463760\n",
            "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 1.461640\n",
            "Train Epoch: 68 [44800/60000 (75%)]\tLoss: 1.461187\n",
            "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 1.461659\n",
            "Train Epoch: 68 [57600/60000 (96%)]\tLoss: 1.464870\n",
            "Train Epoch: 69 [0/60000 (0%)]\tLoss: 1.463454\n",
            "Train Epoch: 69 [6400/60000 (11%)]\tLoss: 1.461536\n",
            "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 1.477753\n",
            "Train Epoch: 69 [19200/60000 (32%)]\tLoss: 1.461632\n",
            "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 1.461690\n",
            "Train Epoch: 69 [32000/60000 (53%)]\tLoss: 1.476650\n",
            "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 1.476863\n",
            "Train Epoch: 69 [44800/60000 (75%)]\tLoss: 1.463421\n",
            "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 1.491542\n",
            "Train Epoch: 69 [57600/60000 (96%)]\tLoss: 1.462433\n",
            "Train Epoch: 70 [0/60000 (0%)]\tLoss: 1.476949\n",
            "Train Epoch: 70 [6400/60000 (11%)]\tLoss: 1.461488\n",
            "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 1.492533\n",
            "Train Epoch: 70 [19200/60000 (32%)]\tLoss: 1.461812\n",
            "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 1.462026\n",
            "Train Epoch: 70 [32000/60000 (53%)]\tLoss: 1.461484\n",
            "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 1.461271\n",
            "Train Epoch: 70 [44800/60000 (75%)]\tLoss: 1.474831\n",
            "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 1.461245\n",
            "Train Epoch: 70 [57600/60000 (96%)]\tLoss: 1.464638\n",
            "Train Epoch: 71 [0/60000 (0%)]\tLoss: 1.461154\n",
            "Train Epoch: 71 [6400/60000 (11%)]\tLoss: 1.461383\n",
            "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 1.462178\n",
            "Train Epoch: 71 [19200/60000 (32%)]\tLoss: 1.462378\n",
            "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 1.463116\n",
            "Train Epoch: 71 [32000/60000 (53%)]\tLoss: 1.463935\n",
            "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 1.469228\n",
            "Train Epoch: 71 [44800/60000 (75%)]\tLoss: 1.476392\n",
            "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 1.477419\n",
            "Train Epoch: 71 [57600/60000 (96%)]\tLoss: 1.461737\n",
            "Train Epoch: 72 [0/60000 (0%)]\tLoss: 1.461474\n",
            "Train Epoch: 72 [6400/60000 (11%)]\tLoss: 1.461650\n",
            "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 1.476583\n",
            "Train Epoch: 72 [19200/60000 (32%)]\tLoss: 1.497424\n",
            "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 1.479469\n",
            "Train Epoch: 72 [32000/60000 (53%)]\tLoss: 1.477368\n",
            "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 1.493868\n",
            "Train Epoch: 72 [44800/60000 (75%)]\tLoss: 1.461239\n",
            "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 1.463020\n",
            "Train Epoch: 72 [57600/60000 (96%)]\tLoss: 1.461188\n",
            "Train Epoch: 73 [0/60000 (0%)]\tLoss: 1.461219\n",
            "Train Epoch: 73 [6400/60000 (11%)]\tLoss: 1.462452\n",
            "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 1.461323\n",
            "Train Epoch: 73 [19200/60000 (32%)]\tLoss: 1.461262\n",
            "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 1.461204\n",
            "Train Epoch: 73 [32000/60000 (53%)]\tLoss: 1.461156\n",
            "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 1.461675\n",
            "Train Epoch: 73 [44800/60000 (75%)]\tLoss: 1.463195\n",
            "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 1.478091\n",
            "Train Epoch: 73 [57600/60000 (96%)]\tLoss: 1.461351\n",
            "Train Epoch: 74 [0/60000 (0%)]\tLoss: 1.476627\n",
            "Train Epoch: 74 [6400/60000 (11%)]\tLoss: 1.461181\n",
            "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 1.463685\n",
            "Train Epoch: 74 [19200/60000 (32%)]\tLoss: 1.468959\n",
            "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 1.507970\n",
            "Train Epoch: 74 [32000/60000 (53%)]\tLoss: 1.461588\n",
            "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 1.472461\n",
            "Train Epoch: 74 [44800/60000 (75%)]\tLoss: 1.461527\n",
            "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 1.461264\n",
            "Train Epoch: 74 [57600/60000 (96%)]\tLoss: 1.461381\n",
            "Train Epoch: 75 [0/60000 (0%)]\tLoss: 1.461164\n",
            "Train Epoch: 75 [6400/60000 (11%)]\tLoss: 1.463072\n",
            "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 1.461325\n",
            "Train Epoch: 75 [19200/60000 (32%)]\tLoss: 1.462073\n",
            "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 1.461370\n",
            "Train Epoch: 75 [32000/60000 (53%)]\tLoss: 1.461331\n",
            "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 1.461719\n",
            "Train Epoch: 75 [44800/60000 (75%)]\tLoss: 1.461154\n",
            "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 1.461853\n",
            "Train Epoch: 75 [57600/60000 (96%)]\tLoss: 1.461578\n",
            "Train Epoch: 76 [0/60000 (0%)]\tLoss: 1.461995\n",
            "Train Epoch: 76 [6400/60000 (11%)]\tLoss: 1.462880\n",
            "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 1.461518\n",
            "Train Epoch: 76 [19200/60000 (32%)]\tLoss: 1.461407\n",
            "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 1.471378\n",
            "Train Epoch: 76 [32000/60000 (53%)]\tLoss: 1.476735\n",
            "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 1.463746\n",
            "Train Epoch: 76 [44800/60000 (75%)]\tLoss: 1.462131\n",
            "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 1.468239\n",
            "Train Epoch: 76 [57600/60000 (96%)]\tLoss: 1.461384\n",
            "Train Epoch: 77 [0/60000 (0%)]\tLoss: 1.462847\n",
            "Train Epoch: 77 [6400/60000 (11%)]\tLoss: 1.461315\n",
            "Train Epoch: 77 [12800/60000 (21%)]\tLoss: 1.461300\n",
            "Train Epoch: 77 [19200/60000 (32%)]\tLoss: 1.461160\n",
            "Train Epoch: 77 [25600/60000 (43%)]\tLoss: 1.461679\n",
            "Train Epoch: 77 [32000/60000 (53%)]\tLoss: 1.477086\n",
            "Train Epoch: 77 [38400/60000 (64%)]\tLoss: 1.461176\n",
            "Train Epoch: 77 [44800/60000 (75%)]\tLoss: 1.476912\n",
            "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 1.461559\n",
            "Train Epoch: 77 [57600/60000 (96%)]\tLoss: 1.461335\n",
            "Train Epoch: 78 [0/60000 (0%)]\tLoss: 1.461322\n",
            "Train Epoch: 78 [6400/60000 (11%)]\tLoss: 1.461407\n",
            "Train Epoch: 78 [12800/60000 (21%)]\tLoss: 1.461336\n",
            "Train Epoch: 78 [19200/60000 (32%)]\tLoss: 1.462065\n",
            "Train Epoch: 78 [25600/60000 (43%)]\tLoss: 1.461267\n",
            "Train Epoch: 78 [32000/60000 (53%)]\tLoss: 1.479534\n",
            "Train Epoch: 78 [38400/60000 (64%)]\tLoss: 1.461514\n",
            "Train Epoch: 78 [44800/60000 (75%)]\tLoss: 1.477132\n",
            "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 1.477160\n",
            "Train Epoch: 78 [57600/60000 (96%)]\tLoss: 1.476873\n",
            "Train Epoch: 79 [0/60000 (0%)]\tLoss: 1.464755\n",
            "Train Epoch: 79 [6400/60000 (11%)]\tLoss: 1.478573\n",
            "Train Epoch: 79 [12800/60000 (21%)]\tLoss: 1.461346\n",
            "Train Epoch: 79 [19200/60000 (32%)]\tLoss: 1.461307\n",
            "Train Epoch: 79 [25600/60000 (43%)]\tLoss: 1.461282\n",
            "Train Epoch: 79 [32000/60000 (53%)]\tLoss: 1.461715\n",
            "Train Epoch: 79 [38400/60000 (64%)]\tLoss: 1.463030\n",
            "Train Epoch: 79 [44800/60000 (75%)]\tLoss: 1.461527\n",
            "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 1.508115\n",
            "Train Epoch: 79 [57600/60000 (96%)]\tLoss: 1.476913\n",
            "Train Epoch: 80 [0/60000 (0%)]\tLoss: 1.479057\n",
            "Train Epoch: 80 [6400/60000 (11%)]\tLoss: 1.461936\n",
            "Train Epoch: 80 [12800/60000 (21%)]\tLoss: 1.466216\n",
            "Train Epoch: 80 [19200/60000 (32%)]\tLoss: 1.463237\n",
            "Train Epoch: 80 [25600/60000 (43%)]\tLoss: 1.461324\n",
            "Train Epoch: 80 [32000/60000 (53%)]\tLoss: 1.461168\n",
            "Train Epoch: 80 [38400/60000 (64%)]\tLoss: 1.480358\n",
            "Train Epoch: 80 [44800/60000 (75%)]\tLoss: 1.461433\n",
            "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 1.461185\n",
            "Train Epoch: 80 [57600/60000 (96%)]\tLoss: 1.461327\n",
            "Train Epoch: 81 [0/60000 (0%)]\tLoss: 1.461285\n",
            "Train Epoch: 81 [6400/60000 (11%)]\tLoss: 1.464277\n",
            "Train Epoch: 81 [12800/60000 (21%)]\tLoss: 1.476838\n",
            "Train Epoch: 81 [19200/60000 (32%)]\tLoss: 1.461219\n",
            "Train Epoch: 81 [25600/60000 (43%)]\tLoss: 1.461189\n",
            "Train Epoch: 81 [32000/60000 (53%)]\tLoss: 1.461222\n",
            "Train Epoch: 81 [38400/60000 (64%)]\tLoss: 1.462786\n",
            "Train Epoch: 81 [44800/60000 (75%)]\tLoss: 1.464185\n",
            "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 1.461204\n",
            "Train Epoch: 81 [57600/60000 (96%)]\tLoss: 1.506489\n",
            "Train Epoch: 82 [0/60000 (0%)]\tLoss: 1.477284\n",
            "Train Epoch: 82 [6400/60000 (11%)]\tLoss: 1.461216\n",
            "Train Epoch: 82 [12800/60000 (21%)]\tLoss: 1.461552\n",
            "Train Epoch: 82 [19200/60000 (32%)]\tLoss: 1.469033\n",
            "Train Epoch: 82 [25600/60000 (43%)]\tLoss: 1.462098\n",
            "Train Epoch: 82 [32000/60000 (53%)]\tLoss: 1.462593\n",
            "Train Epoch: 82 [38400/60000 (64%)]\tLoss: 1.476959\n",
            "Train Epoch: 82 [44800/60000 (75%)]\tLoss: 1.462163\n",
            "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 1.461408\n",
            "Train Epoch: 82 [57600/60000 (96%)]\tLoss: 1.461899\n",
            "Train Epoch: 83 [0/60000 (0%)]\tLoss: 1.477178\n",
            "Train Epoch: 83 [6400/60000 (11%)]\tLoss: 1.461177\n",
            "Train Epoch: 83 [12800/60000 (21%)]\tLoss: 1.476900\n",
            "Train Epoch: 83 [19200/60000 (32%)]\tLoss: 1.476441\n",
            "Train Epoch: 83 [25600/60000 (43%)]\tLoss: 1.461211\n",
            "Train Epoch: 83 [32000/60000 (53%)]\tLoss: 1.461355\n",
            "Train Epoch: 83 [38400/60000 (64%)]\tLoss: 1.461156\n",
            "Train Epoch: 83 [44800/60000 (75%)]\tLoss: 1.461706\n",
            "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 1.461298\n",
            "Train Epoch: 83 [57600/60000 (96%)]\tLoss: 1.461243\n",
            "Train Epoch: 84 [0/60000 (0%)]\tLoss: 1.476453\n",
            "Train Epoch: 84 [6400/60000 (11%)]\tLoss: 1.462477\n",
            "Train Epoch: 84 [12800/60000 (21%)]\tLoss: 1.477053\n",
            "Train Epoch: 84 [19200/60000 (32%)]\tLoss: 1.461761\n",
            "Train Epoch: 84 [25600/60000 (43%)]\tLoss: 1.476923\n",
            "Train Epoch: 84 [32000/60000 (53%)]\tLoss: 1.461196\n",
            "Train Epoch: 84 [38400/60000 (64%)]\tLoss: 1.461346\n",
            "Train Epoch: 84 [44800/60000 (75%)]\tLoss: 1.461327\n",
            "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 1.461411\n",
            "Train Epoch: 84 [57600/60000 (96%)]\tLoss: 1.461395\n",
            "Train Epoch: 85 [0/60000 (0%)]\tLoss: 1.461376\n",
            "Train Epoch: 85 [6400/60000 (11%)]\tLoss: 1.462495\n",
            "Train Epoch: 85 [12800/60000 (21%)]\tLoss: 1.461395\n",
            "Train Epoch: 85 [19200/60000 (32%)]\tLoss: 1.461225\n",
            "Train Epoch: 85 [25600/60000 (43%)]\tLoss: 1.464187\n",
            "Train Epoch: 85 [32000/60000 (53%)]\tLoss: 1.461190\n",
            "Train Epoch: 85 [38400/60000 (64%)]\tLoss: 1.462564\n",
            "Train Epoch: 85 [44800/60000 (75%)]\tLoss: 1.461776\n",
            "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 1.461174\n",
            "Train Epoch: 85 [57600/60000 (96%)]\tLoss: 1.461720\n",
            "Train Epoch: 86 [0/60000 (0%)]\tLoss: 1.461833\n",
            "Train Epoch: 86 [6400/60000 (11%)]\tLoss: 1.461191\n",
            "Train Epoch: 86 [12800/60000 (21%)]\tLoss: 1.476974\n",
            "Train Epoch: 86 [19200/60000 (32%)]\tLoss: 1.476817\n",
            "Train Epoch: 86 [25600/60000 (43%)]\tLoss: 1.461646\n",
            "Train Epoch: 86 [32000/60000 (53%)]\tLoss: 1.462694\n",
            "Train Epoch: 86 [38400/60000 (64%)]\tLoss: 1.461460\n",
            "Train Epoch: 86 [44800/60000 (75%)]\tLoss: 1.477148\n",
            "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 1.497165\n",
            "Train Epoch: 86 [57600/60000 (96%)]\tLoss: 1.461155\n",
            "Train Epoch: 87 [0/60000 (0%)]\tLoss: 1.461334\n",
            "Train Epoch: 87 [6400/60000 (11%)]\tLoss: 1.461314\n",
            "Train Epoch: 87 [12800/60000 (21%)]\tLoss: 1.461636\n",
            "Train Epoch: 87 [19200/60000 (32%)]\tLoss: 1.461574\n",
            "Train Epoch: 87 [25600/60000 (43%)]\tLoss: 1.461170\n",
            "Train Epoch: 87 [32000/60000 (53%)]\tLoss: 1.476140\n",
            "Train Epoch: 87 [38400/60000 (64%)]\tLoss: 1.461229\n",
            "Train Epoch: 87 [44800/60000 (75%)]\tLoss: 1.462504\n",
            "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 1.476757\n",
            "Train Epoch: 87 [57600/60000 (96%)]\tLoss: 1.461163\n",
            "Train Epoch: 88 [0/60000 (0%)]\tLoss: 1.476855\n",
            "Train Epoch: 88 [6400/60000 (11%)]\tLoss: 1.476609\n",
            "Train Epoch: 88 [12800/60000 (21%)]\tLoss: 1.461182\n",
            "Train Epoch: 88 [19200/60000 (32%)]\tLoss: 1.461154\n",
            "Train Epoch: 88 [25600/60000 (43%)]\tLoss: 1.462606\n",
            "Train Epoch: 88 [32000/60000 (53%)]\tLoss: 1.462095\n",
            "Train Epoch: 88 [38400/60000 (64%)]\tLoss: 1.461164\n",
            "Train Epoch: 88 [44800/60000 (75%)]\tLoss: 1.461609\n",
            "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 1.462196\n",
            "Train Epoch: 88 [57600/60000 (96%)]\tLoss: 1.461333\n",
            "Train Epoch: 89 [0/60000 (0%)]\tLoss: 1.461393\n",
            "Train Epoch: 89 [6400/60000 (11%)]\tLoss: 1.461333\n",
            "Train Epoch: 89 [12800/60000 (21%)]\tLoss: 1.461425\n",
            "Train Epoch: 89 [19200/60000 (32%)]\tLoss: 1.461193\n",
            "Train Epoch: 89 [25600/60000 (43%)]\tLoss: 1.461250\n",
            "Train Epoch: 89 [32000/60000 (53%)]\tLoss: 1.461835\n",
            "Train Epoch: 89 [38400/60000 (64%)]\tLoss: 1.462972\n",
            "Train Epoch: 89 [44800/60000 (75%)]\tLoss: 1.461154\n",
            "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 1.461234\n",
            "Train Epoch: 89 [57600/60000 (96%)]\tLoss: 1.461162\n",
            "Train Epoch: 90 [0/60000 (0%)]\tLoss: 1.464659\n",
            "Train Epoch: 90 [6400/60000 (11%)]\tLoss: 1.461189\n",
            "Train Epoch: 90 [12800/60000 (21%)]\tLoss: 1.461426\n",
            "Train Epoch: 90 [19200/60000 (32%)]\tLoss: 1.477036\n",
            "Train Epoch: 90 [25600/60000 (43%)]\tLoss: 1.461170\n",
            "Train Epoch: 90 [32000/60000 (53%)]\tLoss: 1.461232\n",
            "Train Epoch: 90 [38400/60000 (64%)]\tLoss: 1.462237\n",
            "Train Epoch: 90 [44800/60000 (75%)]\tLoss: 1.479635\n",
            "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 1.468351\n",
            "Train Epoch: 90 [57600/60000 (96%)]\tLoss: 1.476704\n",
            "Train Epoch: 91 [0/60000 (0%)]\tLoss: 1.461708\n",
            "Train Epoch: 91 [6400/60000 (11%)]\tLoss: 1.461181\n",
            "Train Epoch: 91 [12800/60000 (21%)]\tLoss: 1.461586\n",
            "Train Epoch: 91 [19200/60000 (32%)]\tLoss: 1.476133\n",
            "Train Epoch: 91 [25600/60000 (43%)]\tLoss: 1.476746\n",
            "Train Epoch: 91 [32000/60000 (53%)]\tLoss: 1.466677\n",
            "Train Epoch: 91 [38400/60000 (64%)]\tLoss: 1.461592\n",
            "Train Epoch: 91 [44800/60000 (75%)]\tLoss: 1.461226\n",
            "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 1.461264\n",
            "Train Epoch: 91 [57600/60000 (96%)]\tLoss: 1.461673\n",
            "Train Epoch: 92 [0/60000 (0%)]\tLoss: 1.461177\n",
            "Train Epoch: 92 [6400/60000 (11%)]\tLoss: 1.461770\n",
            "Train Epoch: 92 [12800/60000 (21%)]\tLoss: 1.461205\n",
            "Train Epoch: 92 [19200/60000 (32%)]\tLoss: 1.461515\n",
            "Train Epoch: 92 [25600/60000 (43%)]\tLoss: 1.462139\n",
            "Train Epoch: 92 [32000/60000 (53%)]\tLoss: 1.463216\n",
            "Train Epoch: 92 [38400/60000 (64%)]\tLoss: 1.461620\n",
            "Train Epoch: 92 [44800/60000 (75%)]\tLoss: 1.476783\n",
            "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 1.461239\n",
            "Train Epoch: 92 [57600/60000 (96%)]\tLoss: 1.461169\n",
            "Train Epoch: 93 [0/60000 (0%)]\tLoss: 1.476558\n",
            "Train Epoch: 93 [6400/60000 (11%)]\tLoss: 1.476709\n",
            "Train Epoch: 93 [12800/60000 (21%)]\tLoss: 1.461230\n",
            "Train Epoch: 93 [19200/60000 (32%)]\tLoss: 1.461291\n",
            "Train Epoch: 93 [25600/60000 (43%)]\tLoss: 1.476338\n",
            "Train Epoch: 93 [32000/60000 (53%)]\tLoss: 1.478202\n",
            "Train Epoch: 93 [38400/60000 (64%)]\tLoss: 1.461199\n",
            "Train Epoch: 93 [44800/60000 (75%)]\tLoss: 1.463031\n",
            "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 1.461226\n",
            "Train Epoch: 93 [57600/60000 (96%)]\tLoss: 1.463201\n",
            "Train Epoch: 94 [0/60000 (0%)]\tLoss: 1.461942\n",
            "Train Epoch: 94 [6400/60000 (11%)]\tLoss: 1.461185\n",
            "Train Epoch: 94 [12800/60000 (21%)]\tLoss: 1.461194\n",
            "Train Epoch: 94 [19200/60000 (32%)]\tLoss: 1.461285\n",
            "Train Epoch: 94 [25600/60000 (43%)]\tLoss: 1.461184\n",
            "Train Epoch: 94 [32000/60000 (53%)]\tLoss: 1.461748\n",
            "Train Epoch: 94 [38400/60000 (64%)]\tLoss: 1.477868\n",
            "Train Epoch: 94 [44800/60000 (75%)]\tLoss: 1.464613\n",
            "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 1.463908\n",
            "Train Epoch: 94 [57600/60000 (96%)]\tLoss: 1.463181\n",
            "Train Epoch: 95 [0/60000 (0%)]\tLoss: 1.476837\n",
            "Train Epoch: 95 [6400/60000 (11%)]\tLoss: 1.461309\n",
            "Train Epoch: 95 [12800/60000 (21%)]\tLoss: 1.461393\n",
            "Train Epoch: 95 [19200/60000 (32%)]\tLoss: 1.461233\n",
            "Train Epoch: 95 [25600/60000 (43%)]\tLoss: 1.461537\n",
            "Train Epoch: 95 [32000/60000 (53%)]\tLoss: 1.461201\n",
            "Train Epoch: 95 [38400/60000 (64%)]\tLoss: 1.461152\n",
            "Train Epoch: 95 [44800/60000 (75%)]\tLoss: 1.461220\n",
            "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 1.461161\n",
            "Train Epoch: 95 [57600/60000 (96%)]\tLoss: 1.461430\n",
            "Train Epoch: 96 [0/60000 (0%)]\tLoss: 1.461362\n",
            "Train Epoch: 96 [6400/60000 (11%)]\tLoss: 1.461289\n",
            "Train Epoch: 96 [12800/60000 (21%)]\tLoss: 1.461193\n",
            "Train Epoch: 96 [19200/60000 (32%)]\tLoss: 1.461226\n",
            "Train Epoch: 96 [25600/60000 (43%)]\tLoss: 1.461845\n",
            "Train Epoch: 96 [32000/60000 (53%)]\tLoss: 1.461674\n",
            "Train Epoch: 96 [38400/60000 (64%)]\tLoss: 1.461175\n",
            "Train Epoch: 96 [44800/60000 (75%)]\tLoss: 1.461446\n",
            "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 1.461365\n",
            "Train Epoch: 96 [57600/60000 (96%)]\tLoss: 1.461230\n",
            "Train Epoch: 97 [0/60000 (0%)]\tLoss: 1.461288\n",
            "Train Epoch: 97 [6400/60000 (11%)]\tLoss: 1.461493\n",
            "Train Epoch: 97 [12800/60000 (21%)]\tLoss: 1.461387\n",
            "Train Epoch: 97 [19200/60000 (32%)]\tLoss: 1.462888\n",
            "Train Epoch: 97 [25600/60000 (43%)]\tLoss: 1.461237\n",
            "Train Epoch: 97 [32000/60000 (53%)]\tLoss: 1.461186\n",
            "Train Epoch: 97 [38400/60000 (64%)]\tLoss: 1.461158\n",
            "Train Epoch: 97 [44800/60000 (75%)]\tLoss: 1.461285\n",
            "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 1.464767\n",
            "Train Epoch: 97 [57600/60000 (96%)]\tLoss: 1.476475\n",
            "Train Epoch: 98 [0/60000 (0%)]\tLoss: 1.461572\n",
            "Train Epoch: 98 [6400/60000 (11%)]\tLoss: 1.461187\n",
            "Train Epoch: 98 [12800/60000 (21%)]\tLoss: 1.476806\n",
            "Train Epoch: 98 [19200/60000 (32%)]\tLoss: 1.477738\n",
            "Train Epoch: 98 [25600/60000 (43%)]\tLoss: 1.463612\n",
            "Train Epoch: 98 [32000/60000 (53%)]\tLoss: 1.461277\n",
            "Train Epoch: 98 [38400/60000 (64%)]\tLoss: 1.461303\n",
            "Train Epoch: 98 [44800/60000 (75%)]\tLoss: 1.461283\n",
            "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 1.461158\n",
            "Train Epoch: 98 [57600/60000 (96%)]\tLoss: 1.492482\n",
            "Train Epoch: 99 [0/60000 (0%)]\tLoss: 1.476621\n",
            "Train Epoch: 99 [6400/60000 (11%)]\tLoss: 1.461752\n",
            "Train Epoch: 99 [12800/60000 (21%)]\tLoss: 1.462113\n",
            "Train Epoch: 99 [19200/60000 (32%)]\tLoss: 1.461177\n",
            "Train Epoch: 99 [25600/60000 (43%)]\tLoss: 1.461853\n",
            "Train Epoch: 99 [32000/60000 (53%)]\tLoss: 1.461600\n",
            "Train Epoch: 99 [38400/60000 (64%)]\tLoss: 1.461324\n",
            "Train Epoch: 99 [44800/60000 (75%)]\tLoss: 1.477077\n",
            "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 1.461174\n",
            "Train Epoch: 99 [57600/60000 (96%)]\tLoss: 1.461287\n",
            "Train set: Average loss: 0.0229, Accuracy: 59799/60000 (100%)\n",
            "Test set: Average loss: 0.0231, Accuracy: 9899/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk8zHeUTD1wf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}